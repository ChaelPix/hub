UVIT46
 Théorie de la communication
 Chap2: Transmissiondel’information:
 Sources etcodagedesource
 Le Schémagénérald’unechaine detransmission est le suivant:
 Figure 1: Schéma d’un système decommunicationaveccodeursdesourceetdecanalséparés.
 La théorie des communications s’intéresse aux moyens de transmettre une information depuis une
 source jusqu’à un utilisateur (voir. Figure 1).
 • Lanaturedelasourcepeutêtre très variée (voix, signal électromagnétique).
 • Lecanalpeutêtre uneligne téléphonique, une liaison radio ou encore un support magnétique
 ouoptique. Lecanalseragénéralementperturbéparunbruitquidépendradel’environnement
 et de la nature du canal : perturbations électriques par exemple.
 • Le but du codeur de source est de représenter, avant la transmission, la sortie de la source en
 uneséquencebinaire, et cela de la façon la plus économique possible.
 • Lebutducodeurdecanaletdesondécodeurestdereproduireleplusfidèlementpossiblecette
 séquence binaire malgré le passage à travers le canal bruité. Enfin, le décodeur de la source
 devra être capable, à partir de la sortie du canal de restituer de façon acceptable l’information
 fournie par la source.
 D
 ans le but de simplifier l’étude des systèmes de communication, nous étudierons séparément
 les modèles de sources et les modèles de canaux.
 1 Sourcesetcodagedesource(Sourcesandsourceencoding)
 Soit X un alphabet fini de lettres X {x1,x2, ,xN} avec lesquels nous travaillons et soit p(xi) la
 probabilité pour que la lettre xi soit choisie.
 La probabilité p(xi) s’établit à la suite d’une recherche statistique en évaluant la fréquence relative
 d’utilisation de chaque lettre xi dans la transmission du message considéré. Ainsi, transmettre une
 certaine quantité d’information revient à connaître la probabilité d’utilisation de chaque symbole.
 1
UVIT46
 Théorie de la communication
 Définition1:
 L’ensemble formé par {X, xi, p(xi)} s’appelle source discrète.
 Définition2:
 Ondiraquelasource X aunemémoired’ordrem silamieme émissiondelasourcedépenddes
 m 1précédentesémissions. Ainsi:
 • Sim 0lasourceestditesansmémoire
 • Sim 1lasourceestditedeMarkov
 Remarque: L’entropie d’une source de Markov est définie par la moyenne pondérée des entropies
 de tous les états. Si l’entropie des symboles émis dans l’état xi est présentée par H(X/xi), l’entropie
 totale de la source est donnée par :
 H(X) H (X) H(X/X)
 p(xi) H(X/xi)
 i
 Exemple de calcul de l’entropie d’une source de Markov: Considérons la source de Markov X 
{a,b} de graphe :
 0,4
 0,6
 a
 0,75
 b 0,25
 Les flèches indiquent les probabilités conditionnelles de transition d’un état à un autre. Par exemple
 0,75estlaprobabilitéd’émettrelesymbolea sachantquelasourceaémislesymboleb. Ainsi,d’après
 le graphe on a:
 p(X a/X a) 0,6; p(X b/X a) 0,4
 p(X a/X b) 0,75; p(X b/X b) 0,25
 Il vient alors
 H(X/a)
 H(X/b)
 0,6log2(0,6) [0,4log2(0,4) 0,753bit
 0,75log2(0,75) 0,25log2(0,25) 0,778bit
 2
UVIT46
 Théorie de la communication
 Pour calculer p(a) et p(b) il faut résoudre le système suivant:
 p(a) p(a a) p(a b) p(a/a)p(a) p(a/b)p(b) 0,6p(a) 0,75p(b)
 p(b) p(b a) p(b b) p(b/a)p(a) p(b/b)p(b) 0,4p(a) 0,25p(b)
 avec p(a) p(b) 1. Aprèscalculontrouve
 p(a) 15
 23 ; p(b) 8
 23
 Finalement
 H(X) p(a) H(X/a) p(b) H(X/b) 15
 23 0,753 8
 23 0,778 0,761bit
 Définition3:Débitd’informationd’unesource(TheSourceInformationFlow)
 Soit D le nombre de symboles émis par la source en une seconde et soit H(X) l’entropie de la
 source. Le débit d’information, noté H(X) est définie par:
 H(X) D H(X)
 2 Codagedesource(Sourceencoding)
 L’objectif du codage de source est de supprimer les parties redondantes de l’information délivrée par
 la source. Deux types de codes peuvent être utilisés :
 Codeàlongueurfixe: Touslesmotsontlamêmelongueur(mêmenombredesymbolesbinaires).
 Codeàlongueurvariable(compressionsansperted’information): Lalongueurdesmotsvarieen
 fonctiondeleurfréquenced’apparition. Unmotserad’autantpluslongquesaprobabilitéd’apparition
 sera petite. Ce type de codage consiste à réduire le débit binaire d’une source tout en conservant
 l’information fournie par celle-ci.
 2.1 Généralitéssurlescodes
 Soit X unesourcedeN symbolesxi etdeprobabilitésp(xi). Soit A,unalphabetderéférence(générale
ment binaire, A {0,1}). Coder l’information exprimée par les symboles de X à l’aide des symboles
 de A revient à réaliser une correspondance entre X et l’ensemble des séquences de symboles de A.
 Donc, àchaquesymbolede X onassocieuneséquencedesymbolede A.
 Onnoteracard(A) Q lataillede A etn(xi)lalongueurdumotcodedusymbole xi.
 3
UVIT46
 Théorie de la communication
 Définition4:Longueurmoyenned’uncode(averagecodelength)
 La longueur moyenne, notée n,d’uncodeC estdéfiniepar
 N
 n(C)
 n(xi) p(xi)
 1
 C’est la longueur moyenne de codage d’unsymbolede X.
 Exemple : X {a,b,c,d} avec p(a) 1/2, p(b) 1/4, p(c) p(d) 1/8. A {0,1}Onpropose de
 coder la source de 3 façons selon le tableau suivant
 C1 C2 C3
 a 00 000 1
 b 01 01 01
 c
 10 10 001
 d 11 1 000
 Alors on a card(A) Q 2. Pourchaquecode,lalongueurmoyennepourcoderunelettresera
 n(C1) 1/2 2 1/4 2 1/8 2 1/8 2 2
 n(C2) 1/2 3 1/4 2 1/8 2 1/8 1 2,375
 n(C3) 1/2 1 1/4 2 1/8 3 1/8 3 1,75
 Remarques:
 • LecodeC3 estle meilleur des 3 codes: c’est une compression par rapport aux 2 bits (symboles
 binaires) nécessaires pour coder 22 4 symboles.
 • LecodeC2 n’estpasun"boncode"parcequ’ilnepermetpasundéchiffrageunique;parexem
ple 101 db ou101 cd
 Questions: Parsuiteàcetexemple,lesquestionsquiseposentsontlessuivantes:
 Q1: Commentconstruire un code uniquement déchiffrable? (How to build an uniquely decodable
 code)
 Q2: Peut-on toujours compresser sans perte d’information le contenu d’une source ? (Existence of
 lossless compression ?)
 4
UVIT46
 Théorie de la communication
 Théorème1:RéponseàlaquestionQ
 1
 Une condition suffisante pour qu’un code soit uniquement déchiffrable est qu’il soit préfixe
 (Prefix code), c’est-à-dire qu’il ne comporte aucun mot code qui soit le début d’un autre mot
 code (nocodewordistheprefixofanyothercodeword).
 Théorème2:kraftinequality
 Uncodepréfixevérifiel’inégalité de Kraft
 N
 1
 Q n(xi) ·1
 Le codeC2 del’exempleprécédent nevérifie pas cette inégalité. En effet,
 2 3 2 2 2 2 2 1 1,125 1
 Théorème3:RéponseàlaquestionQ
 2
 Onse donne une source X d’alphabet de N symboles et d’entropie H(X). Alors sont contenu
 est compressible sans perte d’information si
 H(X) log2(N)
 C’est-à-dire si son entropie n’est pas maximale. On dit alors que la source possède de la re
dondance et l’opération du codage de source va consiste à réduire en partie ou en totalité la
 redondance. Onpeutmesurerlaredondanceenpourcentage par
 R 1 H(X)
 log2(N)%
 2.2 Performancesducodage
 Objectif : Un codage étant d’autant plus économique qu’il fait correspondre les symboles de la
 source X des mots code plus courts. Il en résulte que nous sommes intéressés par le codage dont
 la longueur moyennen estlapluspetite.
 Questions:
 Q3: Commentpeut-onévaluerlapluspetite longueur moyenne n possible. Autrement dit peut-on
 donner uneborneinférieure pour n ?
 5
UVIT46
 Théorie de la communication
 Q4: Existe-t-il un codagecorrespondantàcettelongueurmoyenneminimale. Autrementditexiste
t-il un codage le plus économique et commentpeut-onle réaliser ?
 Théorème4:RéponseàlaquestionQ
 3
 Soit X une source discrète sans mémoire d’entropie H(X) codé en binaire par un code C de
 longueur moyennen(C). Alors nécessairement
 H(X)·n(C)
 Remarque: D’après le théorème 4, n H(X)estla plus petite longueur moyenne en-dessous de
 laquelle il n’est pas possible de descendre. En d’autre termes, il n’existe aucun code dont la longueur
 moyenneeststrictement inférieure à H(X).
 Définition5:
 l’alphabet formé de tous les mots composés de symboles de X de longueur M est appelé
 l’extension d’ordre M et est noté XM.
 Exemple : Soit X {a,b}. L’extension d’ordre 2 de X est X2 {aa,ab,ba,bb}. L’extension d’ordre
 3 de X est X3 {aaa,aab,aba,baa,abb,bab, }
 Théorème5:RéponseàlaquestionQ
 4
 Soit X une source discrète d’entropie H(X). Supposons que l’on effectue le codage non pas à
 chaque symbolede X maisauxsymbolesdesonextensiond’ordre M. Alorsona
 H(X)·n nM
 M ·H(X) 1
 M
 OùnM estla longueur moyenne correspondant au codage des symboles de XM et n nM
 la longueur moyennecorrespondant au codage d’un symbolede X.
 M est
 Remarque: Pourcomprendrecommentlethéorème5répondàlaquestion4ilsuffitderemarquer
 quesi M 
alors 1
 M 
0etparconséquentn 
H(X). End’autretermes,pourtrouverlecode
 qui correspond à la longueur moyenneminimalen H(X)ilfautcoderuneextensionde X d’ordre
 très élevé.
 3 CodedeHuffman:
 LecodagedeHuffmanest"non-destructif",c’estàdirequ’ilcompresselesdonnéessansperted’information,
 de sorte que le fichier décompressé est une copie conforme de l’original. Cet algorithme permet
 6
UVIT46
 Théorie de la communication
 d’attribuer un mot decodebinaireauxdifférents symbolesd’unesource. Lalongueurdechaquemot
 de code n’est pas identique pour tous les symboles: les symboles les plus fréquents sont codés avec
 de petits mots de code binaires, tandis que les symboles les plus rares reçoivent des mots code plus
 longs. On parle de codage àlongueur variable.
 Le codage de Huffman crée un arbre ordonné à partir de tous les symboles et de leur fréquence
 d’apparition. Lesbranchessontconstruitesrécursivementenpartantdessymboleslesmoinsfréquents.
 Les étapes de construction sont les suivantes:
 1. Onclasse les lettres de la source X par ordre décroissant ou croissant de probabilité.
 2. Onrelie2lettresdeplusfaibleprobabilitépar2arêtespourdonnerunnoeudpèrequiremplace
 ces 2 lettres. Le noeud père se voit attribuer comme probabilité la somme des probabilités
 de ses deux fils. On affecte alors le bit 0 à l’arête de gauche et le bit 1 à l’arête de droite (ou
 l’inverse).
 3. Nous avons un nouvel ensemble de noeuds ayant un élément en moins (remplacement des
 deuxfilsparlepère)etnousreprenonsl’étape1jusqu’àl’arrivéeausommetdel’arbredeprob
abilité 1 (ensemble de noeuds d’unseul élément). Onconstruit de cette manière unarbre dont
 les feuilles sont les symboles à coder et les embranchementslescodagesintermédiaires. lemot
 coded’unsymbolen’estautrequelecodeducheminmenantdelaracineàlafeuilleassociéeà
 ce symbole.
 Exemple1 Considéronslasourcediscrète sans mémoire X {A,B,C,D}avec p(A) 1/3, p(B)
 1/2, p(C) p(D) 1/12
 Premièreitération:
 • les symboles peuprobables sontC etD
 • créer unnouveaunoeud(CD;1/6)
 • ajouter unnouveausymboleàlasource X {(A;1/3),(B;1/2),(CD;1/6)}.
 0
 CD
 C
 Deuxièmeitération:
 • les symboles peuprobables sontCD et A
 • créer unnouveaunoeud(ACD;1/2)
 1
 D
 • ajouter unnouveausymboleàlasource X {(B;1/2),(ACD;1/2)}.
 7
UVIT46
 ACD
 Théorie de la communication
 0
 A
 Troisième itération:
 1
 0
 CD
 C
 • les symboles peuprobables sont ACD etB
 • créer unnouveaunoeud(BACD;1)
 1
 D
 • ajouter unnouveausymboleàlasource X {(BACD;1)}.
 0
 B
 1
 0
 ACD
 A
 Fin del’algorithme
 Les motscodesont:
 1
 0
 CD
 C
 1
 D
 B 0, A 10, C 110, B 111
 4 CodedeFano:utiliséedanslescompressionsJPEGetMPEG
 La méthodedeFanofournit uncodage binaire à la volée. Elle est proche de la méthode de Huffman.
 Les étapes de construction sont les suivantes:
 1. Classer les messages de la source dans l’ordre des probabilités décroissantes.
 2. Diviser l’ensemble des messages en deux groupes de probabilités aussi proches que possibles
 et
 • Attribuer 0 par exemple aupremier
 • Attribuer 1 au second.
 3. Recommencerlessubdivisions successivement.
 8
UVIT46
 Théorie de la communication
 Exemple 2 : Onconsidère une source discrète sans mémoire X {A,B} avec p(A) 0,8, p(B)
 0,2 Proposer un codagedeFanodessymbolesdel’extension X3.
 Solution : Ona
 X3 {AAA,AAB,ABA,BAA,ABB,BAB,BBA,BBB}
 Puisque la source est sans mémoire, on obtient alors
 p(AAA) p(A) p(A) p(A) 0,8 0,8 0,8 0,512
 Delamêmemanièreoncalculelesprobabilitésdesautressymboles. Lestableauxsuivantsmontrent
 les étapes du codage.
 9
UVIT46
 Théorie de la communication
 10
UVIT46
 Théorie de la communication
 Codage: Letableausuivantprésente le codage final de Fano.
 Exemple3: OnconsidèrelasourcedeMarkov X {A,B}degraphe
 1
 2
 1
 2
 A
 1
 1. Calculer p(A) et p(B) (aide: voir page 3)
 2. Calculer H(X)
 3. Oncodedirectementles symboles delasource par:
 A
 0; B 1
 B
 Calculer la longueur moyenne du code etcomparerla avec H(X).
 4. Au lieu de coder directement les symboles de X, On désire coder les symboles de l’extension
 d’ordre 2 de X. En effet, d’après le graphe on remarque que le symbole BB est impossible.
 Ainsi, les seuls symboles possibles de X2 sont:
 X2 {AA, AB,BA}
 4.a) Calculer
 p(AA), p(AB), p(BA)
 11
UVIT46
 Théorie de la communication
 4.b) Proposer un codage de Huffman des symboles de l’extension X2 et calculer sa longueur
 moyennen2. Endéduire la longueur moyenne n correspondant au codage d’un symbole
 de X.
 4.c) Commenterlesrésultats.
 Solution
 1. Pourcalculer p(A) et p(B) il faut résoudre le système suivant:
 p(A) 0,5p(A) p(B)
 p(B) 0,5p(A)
 avec p(A) p(B) 1. Aprèscalculontrouve
 p(A) 2
 3 ; p(B) 1
 3
 2. D’après le graphe on a:
 Il vient alors
 p(X A/X A) 0,5; p(X B/X A) 0,5
 p(X A/X B) 1; p(X B/X B) 0
 H(X/A)
 Finalement
 0,5log2(0,5) 0,5log2(0,5) 1bit
 H(X/B) 1 log2(1) 0bit
 H(X) p(A) H(X/A) p(B) H(X/B) 2
 3 1 1
 3 0 0,66bit
 3. n 2
 3 1 1
 3 1 1 0,66 H(X)
 4.a) Ona
 p(AA) p(X A X A) p(A) p(X A/X A) 2
 3
 1
 2
 1
 3
 p(AB) p(X B X A) p(A) p(X B/X A) 2
 3
 1
 2
 1
 3
 p(BA) p(X B X A) p(B) p(X A/X B) 1
 3 1 1
 3
 12
UVIT46
 Théorie de la communication
 4.b) L’application de l’algorithme de Huffman conduit à
 1
 0
 AA:1/3
 1
 0
 2/3
 AB:1/3
 Les motscodesontalors:
 1
 BA:1/3
 AA 0, AB 10, BA 11
 Ainsi, la longueur moyenne n2 de l’extension d’ordre 2 de X est donnée par:
 n2 
1
 3 1 1
 3 2 1
 3 2 5
 3 1,66bits
 Onendéduitlalongueurmoyennecorrespondantaucodaged’unsymbolede X
 n n2
 2 
1,66
 2 
0,83bit
 Onenconclutquelecodagedel’extensiond’ordre2de X apermisd’améliorerlalongueurmoyenne
 correspondant au codage d’un symbole de X.
 13
UVIT46 Théoriedelacommunication
 TD2
 Exercice1: Vérifiers’ilexisteuncodebinaireuniquementdéchiffrabledelongueursdemotssuiv
antes:
 {4,4,4,3,2,4,3,4,3,4}
 Solution: Ilsuffitd’appliquerl’inégalitédeKraft N
 1Q n(xi)·1.Eneffet,onaQ 2etN 10. Il
 s’ensuit:
 6 2 4 3 2 3 1 2 2 1
 L’inégalitédeKraftestdoncvérifiée.Onendéduitqu’untelcodepeutexister.
 Exercice2: UnesourceXsansmémoireetsoncodagesontdonnésdansletableausuivant:
 X x1 0 x2 1 x3 2 x4 3
 p(X) 0.5 0.25 0.125 0.125
 codage 0 10 110 111
 1. Calculerl’entropiedelasource.
 2. Lecodeest-iluniquementdéchiffrable?
 3. Calculersalongueurmoyenne.
 4. Lecodeest-iloptimal?
 5. Vérifierl’inégalitédeKraft.
 6. Coder0312201.
 7. Décoder1001101010.
 Solution:
 1. H(X) 1,75bits
 2. Aucunmotdecoden’estpréfixed’unautre.Cecodeestdoncuniquementdéchiffrable.
 3. n 0,5 1 2 0,25 3 0:125 3 0:125 1:75bits
 4. Puisquen H(X)alorscecodeestoptimal.
 5. 1
 21
 1
 22
 1
 23
 1
 23
 1
 6. 0312201 011110110110010
 7. 1001101010 10211
 14
UVIT46
 Théorie de la communication
 Exercice 3: Unesourced’information ternaire X {0,1,2} est modélisée par une chaîne de Markov
 dont le graphe est :
 1/2
 0
 1/2
 1
 2
 1/2
 1
 1/2
 1. X délivre 3000 symboles par seconde. Calculer, en bits par seconde, le débit d’information de
 X.
 2. Effectuer uncodagedeHuffmandel’extensiond’ordre2de X. Endéduirelenombremoyende
 bits utilisés pour coder un symbole de X.
 Solution :
 1. Le débit d’entropie (d’information) de X s’écrit H(X) D H(X). Or D 3000 symboles par
 seconde. Il reste à calculer H(X). En effet, D’après le graphe on a:
 p(X 0/X 0) 0; p(X 1/X 0) 1, p(X 2/X 0) 0
 p(X 0/X 1) 0,5; p(X 1/X 1) 0, p(X 2/X 1) 0,5
 p(X 0/X 2) 0,5; p(X 1/X 2) 0, p(X 2/X 2) 0,5
 Oronsait que
 et
 H(X) p(0)H(X/X 0) p(1)H(X/X 1) p(2)H(X/X 2)
 H(X/X 0) 1 log2(1) 0bit
 H(X/X 1) 0,5log2(0,5) 0,5log2(0,5) 1bit
 H(X/X 2) 0,5log2(0,5) 0,5log2(0,5) 1bit
 15
UVIT46
 Théorie de la communication
 Calcul de p(0), p(1)et p(2): Il faut résoudre de système suivant:
 p(0) 0,5p(1) 0,5p(2)
 p(1) p(0)
 p(2) 0,5p(1) 0,5p(2)
 avec p(0) p(1) p(2) 1. Aprèscalculontrouve
 p(0) p(1) p(2) 1
 3
 Finalement
 H(X) p(0)H(X/X 0) p(1)H(X/X 1) p(2)H(X/X 2)
 1
 3 0 1
 3 1 1
 3 1 0,666bit
 Le débit d’information est donc
 H(X) D H(X) 3000 0,666 2Kbits/sec
 2. Pour effectuer le codage de Huffman de l’extension d’ordre 2 de X, il faut constituer tous les
 motspossibles formésdedeuxlettres source enleuraffectant leurs probabilités respectives. Le
 tableau suivant résume les calculs:
 L’application de l’algorithme de Huffman conduit à :
 2/3
 0
 1/3
 1
 0
 1
 1/3
 0
 22:p 1/6
 1
 20:p 1/6
 0
 12:p 1/6
 1
 1
 01:p 1/3
 10:p 1/6
 Onobtient donc:
 16
UVIT46
 Théorie de la communication
 La longueur moyenned’unmotcodedel’extensiond’ordre 2 s’en déduit aisément :
 n2 
1
 3 1 4 1
 6 3 2,333bits
 Ainsi, le nombre moyendebits utilisés pour coder les valeurs de X est donc :
 n n2
 2 
2,333
 2 
1,166bits
 Onenconclut que le codage de l’extension d’ordre 2 de X n’est pas suffisant pour atteindre la
 valeur optimale H(X).
 Exercice4: Unchariotautomatiqueeffectueuntrajetencircuitauseind’uncentredetripostal. La
 vitesse du chariot ne peut prendre que quatre valeurs possibles v0,v1,v2,v3.
 Pour pouvoir déterminer les causes d’un incident, on souhaite incorporer dans ce chariot une mé
moire vive de 4096 octets afin d’y enregistrer la vitesse du chariot à chaque seconde pendant une
 journée complète de 8 heures. La vitesse du chariot a pour cela été observée toutes les secondes
 pendant une période de fonctionnement normal, et une étude statistique a permis de construire les
 probabilités de transition suivantes :
 p(v0/v0) 0.9
 p(v1/v0) 0.1
 p(v0/v1) 0.4
 p(v1/v1) 0.4
 p(v2/v1) 0.2
 p(v1/v2) 0.4
 p(v2/v2) 0.5
 p(v3/v2) 0.1
 p(v2/v3) 0.6
 p(v3/v3) 0.4
 1. Représenter le diagramme de Markov correspondant à la modélisation proposée du chariot,
 assimilé à une source X émettant unmessageconstruit à l’aide des symboles v0,v1,v2,v3.
 2. Ensachantquelesprobabilités des vitesses sont les suivantes : p(v0) 0.7165; p(v1) 0.1791;
 p(v2) 0.0896; p(v3) 0.0149,calculer l’entropie de la source. En déduire sa redondance.
 17
UVIT46
 Théorie de la communication
 3. Est-il possible de stocker les valeurs de la vitesse mesurées chaque seconde pendant 8 heures
 dans la mémoirevivede4096octets.
 4. Montrerqu’uncodedelongueurfixeneconvientpas.
 5. Uncodagedirect deHuffmandessymbolesde X convient-il?
 6. Afindefournirunesolution à ceproblème, déterminer l’ordre d’extension de la source X pour
 quele stockage des vitesses soit possible.
 Solution:
 1. Voir figure
 0,4
 0,1
 0,5
 0,2
 0,4
 0,1
 0,9
 v0 v1 v2 v3
 0,4
 0,4
 2. L’entropie de la source est donnée par
 H(X)
 0,6
 p(vi) H(X/vi)
 i
 Calcul de H(X/vi)
 D’après le diagramme onobtient
 H(X/v0)
 de la mêmemanièreonobtient:
 0,9log2(0,9) 0,1log2(0,1) 0,469bit
 H(X/v1) 1,5219bits H(X/v2) 1,361bits H(X/v3) 0,971bit
 Enondéduit:
 H(X) p(v0) H(X/v0) p(v1) H(X/v1) p(v2) H(X/v2) p(v3) H(X/v3)
 0,745bit
 La redondance delasource est :
 R 1 H(X)
 log2(4) 62,7%
 3. Pendant 8h on peut enregistrer 0,745 8 60 60 21455bits. Or on dispose d’une mé
moiredecapacité4096 8 32768bits. Ilestdoncpossibledestockerlesvaleursdelavitesse
 mesurées dans la mémoire.
 18
UVIT46 Théoriedelacommunication
 4. Ilfaudraitaumoins2bitspourcoderchacundes4symboles.Donc,pendant8honenregistre
 :2 8 60 60 57600bits.Commelamémoirenepeutcontenirque32768bits,cecodene
 peutpasconvenir.
 5. LecodagedeHuffmanestlesuivant
 1
 0,2835
 0,1045
 V3:p 0,0149
 0
 v2:p 0,0896
 1
 0
 v1:p 0,179
 1
 0
 v0:p 0,7165
 1
 Lesmotscodedechaquesymbolesont
 v0 1, v1 01, v2 001, v3 000,
 Ainsi,lalongueurmoyenneégaleà
 n 1 p(v0) 2 p(v1) 3 p(v2) 3 p(v3) 1,3882bits
 Donc,pendant8honpeutenregistrer1,3882 8 60 60 39980,16bitsquiestdoncsupérieur
 àlacapacitédelamémoire32768bits.Cecodenepeutpasdoncconvenir.
 6. Lenombredesymbolesenregistréspendant8heuresest8 60 60 28800.Commelamé
moirenepeutcontenirque32768bits,onpourrautilisern’importequelcodedontlalongueur
 moyennenparsymboleestinférieureà32768
 28800 1,138bits.Ord’aprèslethéorèmedeShannon
 ona
 n·H(X) 1
 M
 oùMestl’ordred’extension.Donc,sionimpose
 H(X) 1
 M·1,138
 Onvérifieran·1,138.Ainsi,
 H(X) 1
 M·1,138 M 1
 1,138 H(X) 2,55
 CequiconduitàM 3.
 19
UVIT46
 Théorie de la communication
 Exercice 5 : CodageHuffmanpourlesimages:
 Voici l’image à coder de taille 9 9 pixels
 Onsupposequechaquepixelestcodésur8bits. Cetteimagecontient3couleurs: Noir,GrisetBlanc.
 Si on représente chaque pixel par sa couleur on obtient la matrice des couleurs suivante : On notera
 quela taille de l’image est 9 9 8 648bits.
 Danscet exercice, nous allons appliquer à l’image le codage de Huffman en considérant les couleurs
 commeunalphabet de la source. Pour toute lecture, l’image sera parcourue ligne par ligne en com
mençantparlecoinenhautàgauche.
 La table des fréquences des couleurs de l’image ainsi que leurs probabilités sont données par .
 Couleur
 B
 N
 Nombred’occurrences
 40
 G
 13
 Probabilité
 28
 40/81 13/81 28/81
 Chaquepixelétantcodésur8bits,l’imageoccupedoncunetaillede: 9 9 8 648bits 81Octets.
 1. Calculer l’entropie de l’image. En déduire sa redondance.
 2. A partir de la table des fréquences, déterminer le codage de Huffman. En déduire la longueur
 moyenneducodagedeHuffman:
 3. Calculer la taille de l’image du code de Huffman. Quel est le taux de compression ?
 20
UVIT46 Théoriedelacommunication
 Solution:
 1. Calculdel’entropie:
 H 40
 81 log2
 40
 81
 13
 81 log2
 13
 81
 28
 81 log2
 28
 81 1,46bit
 D’oùlaredondance:
 R 1 H(X)
 log2(3) 1 1,46
 log2(3) 0,0812
 2. ArbredeHuffman
 1
 B:40/81
 0
 41/81
 G:28/81
 0
 N:13/81
 1
 1
 Tableducodage
 Couleur B G N
 Motcode 0 10 11
 Probabilité 40/81 28/81 13/81
 LongueurmoyenneducodagedeHuffman:
 n 1 40
 81 2 28
 81 2 13
 81
 122
 81 1,5bits
 3. Latailledel’imageducodedeHuffmanestde1 40 2 28 2 13 122bits.Latailledu
 l’imageinitialeestde648bits.Letauxdecompressionestdoncdonnépar
 t 1 Tailledesdonnéescompressées
 Tailledesdonnéesnoncompressées 1 122
 648 81%
 21