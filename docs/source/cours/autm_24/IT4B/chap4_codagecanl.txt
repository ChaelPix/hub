UV IT4B Théorie de la communication
Chap 4: Codage de canal: Codes détecteurs et correcteurs d’erreurs
Principe du codage de canal: D’après le chapitre 3, l’information transmise par le canal est donnée
par :
I(X ,Y ) = H(X )− H(X /Y )
• H(X ) représente l’information de chaque symbole source transmise par un canal sans bruit.
• H(X /Y ) représente la perte d’information due au bruit. C’est le nombre d’éléments binaires
qui manquent à la réception. Ce nombre représente donc l’information additionnelle qui est
requise pour supprimer l’ambiguïté.
Le principe du codage de canal consiste donc à introduire de la redondance dans la
séquence d’information binaire (mots code source) afin de corriger les erreurs de transmission durant la réception de l’information.
Préparation
Pour simplifier, on ne parlera que des codes binaires. Rappelons d’abord que le corps a deux éléments
{0,1} est muni de l’addition binaire le OU-exclusif, souvent appelé XOR :
0⊕0 = 0, 0⊕1 = 1, 1⊕1 = 0
et de la multiplication usuelle
0×0 = 0, 0×1 = 0, 1×1 = 1
La table de vérité du XOR et le symbole associés à cette fonction sont :
A
B
X = A ⊕B
A B X = A ⊕B
0 0 0
0 1 1
1 0 1
1 1 0
1
UV IT4B Théorie de la communication
Calculs de probabilités : La situation à modéliser est la suivante : on transmet dans un CBS un
message composé de n bits. Pendant la transmission, chaque bit est modifié avec la probabilité p <
0,5 (c’est le taux d’erreur du canal de transmission : on a p ' nombre de bits erronés/nombre de bits
transmis).
X Y
0 0
1 1
1− p
1− p
p
p
On fait les hypothèses que les erreurs sont indépendantes pour chaque bit. Il s’agit d’un schéma de
Bernouilli, et la loi de probabilité correspondante est une loi binomiale de paramètres n et p :
• Variable aléatoire X = nombre de bits erronés. Les valeurs possibles pour X sont {0,1,2,··· ,n}.
• Probabilité de recevoir un message avec h bits erronés : p(X = h) =
¡n
h
¢
p
h
(1 − p)
n−h
, avec
¡n
h
¢
=
n!
h! (n−h)! les coefficients binomiaux.
Exemple : Supposons qu’on utilise un code de longueur 3 avec un canal CBS de probabilité d’erreur
p = 0,001 (dans la réalité, ce serait un canal de très mauvaise qualité).
1. Probabilité de recevoir un message avec h bits erronés :
p(X = h) =
Ã
3
h
!
0,001h
(0,999)3−h
2. Probabilité que le message reçu ne comporte pas d’erreur :
p(X = 0) = 1×0,0010 ×0;9993 ' 99,7%
3. Probabilité que le message reçu comporte 1 erreur :
p(X = 1) = 3×0,0011 ×0;9992 ' 0,29%
4. Probabilité que le message reçu comporte 2 erreur :
p(X = 2) =
3×2
2
×0,0012 ×0;9901 ' 0,000003%
2
UV IT4B Théorie de la communication
5. Probabilité que le message reçu comporte 3 erreur :
p(X = 3) = 1×0,0013 ×0;9990 ' 0,000000001%
Le calcul des probabilités permet d’affirmer :
• Environ 99,7% des messages reçus ne comportent pas d’erreur;
• Environ 0,29% des messages reçus comportent une erreur ;
• Environ 0,000003% des messages reçus comportent deux erreurs ;
• Environ 0,000000001% des messages reçus comportent trois erreurs.
Conclusion
• Il est beaucoup plus probable de recevoir un message sans erreurs qu’avec beaucoup
d’erreurs.
• Quand on ne détecte aucune erreur dans le message reçu, il est très probable qu’il ne
contienne effectivement aucune erreur.
• Si le mot reçu est erroné, il a y plus de chance qu’il comporte 1 ou peu d’erreurs qu’avec
beaucoup d’erreurs. Ainsi, quand on corrige un message erroné par le mot de code le
plus proche (c-à-d avec le moins de bits différents), il est très probable qu’il soit corrigé
correctement.
3
UV IT4B Théorie de la communication
1 Codes détecteurs et correcteurs d’erreurs
On s’intéressera au codage par blocs : chaque mot source de longueur k (éléments binaires) sera
codé par un mot de longueur n > k. Ainsi, les mots code du canal sont formés de k bits d’information
(mots code source binaire ) et r bits de contrôle, c’est-à-dire
Mot code
| {z }
Codage canal
= ( i1,i2,··· ,ik
| {z }
k bits d’information
= mot source
, c1,c2,··· ,cr
| {z }
r bits de contôle
), n = k +r
On aura donc le schéma de transmission suivant:
– Codage de source :
Toto
|{z}
Mot source
=⇒
Codage de source
10110011000011110011 =⇒
Découpage en blocs
1011
¯
¯ 0011
¯
¯ 0000
¯
¯ 1111
¯
¯ 0011
| {z }
Mots information k = 4
– Codage de canal : chaque bloc est codé séparément avec ajout de bits de contrôle.
(i1,i2,··· ,ik )
| {z }
Mot informatio
=⇒
Codage de canal
(i1,i2,··· ,ik , c1,c2,··· ,cr )
| {z }
Mot code
On parle alors d’un code C(n,k)
Ainsi, l’algorithme du codage par blocs suit les étapes suivantes:
1. Découper le message source binaire en blocs (paquets de bits) de longueur k.
2. Coder séparément chaque bloc avec ajout de bits de contrôle : On forme ainsi les mots de code.
3. Envoi successive des mots de code.
Question : Sous quelle condition existe-il un codage canal avec une probabilité d’erreur après
décodage aussi petite que souhaitée?
Soit un canal de capacité C . Soit ² > 0 arbitraire. Il existe au moins un code C(n,k) de probabilité d’erreur après décodage pe < ² si et seulement si
k
n
< C
Théorème 1 : Deuxième théorème de Shannon
4
UV IT4B Théorie de la communication
Ce théorème donne une condition nécessaire sur le nombre minimum rmin de bits de contrôle
à ajouter :
r > k
µ
1
C
−1
¶
= rmin
Cette borne ne dépend que de la capacité du canal.
Remarque 1 :
2 Définitions et préparation pour les codes linéaires
Soient {0,1}
n
l’ensemble des mots de longueur n sur {0,1}, x = (x1,··· ,xn) et y = (y1,··· , yn) des mots
de {0,1}
n
.
le poids de Hamming, noté w(x) d’un mot x ∈ {0,1}
n
est le nombre de ses coordonnées non
nulles.
Définition 1 : Poids de Hamming : Hamming Weight
Exemple: w(0,1,1,0,1) = 3
notée dH (x, y), est le nombre total d’indices j tels que xj 6= y j
, c-à-d
dH (x, y) = w(x + y)
Définition 2 : Distance de Hamming : Hamming distance
Exemple: dH
³
(0,1,1,0,1); (1,1,1,0,0)´
= 2. En effet,
(0,1,1,0,1)+(1,1,1,0,0) = (1,0,0,0,1) =⇒ w(1,0,0,0,1) = 2
C’est le poids du mot le plus léger que l’on puisse y trouver, le mot (0,0,··· ,0) est mis à part,
c’est-à-dire
wmin = min{w(x); x 6= 0 ∈C}
Définition 3 : Poids minimum d’un code : Minimum Hamming Weight
5
UV IT4B Théorie de la communication
On appelle distance minimale d’un code C, l’entier
dmin = min{dH (x, y); x, y ∈C}
Définition 4 : Distance minimale d’un code : Minimum Hamming distance
Un code C(n,k) sur {0,1} permet de détecter toutes les configurations de t-erreurs ssi:
dmin > t
Théorème 2 : Détection d’erreurs : Minimum Hamming distance for error detection
Un code C(n,k) sur {0,1} permet de corriger toutes les configurations de t-erreurs ssi:
dmin > 2t
Théorème 3 : Correction d’erreurs : Minimum Hamming distance for error correction
Un code C(n,k) t-correcteur, c-à-d permet de corriger toutes les configurations de t-erreurs si
et seulement si
Ã
n
0
!
+
Ã
n
1
!
+
Ã
n
2
!
+···Ã
n
t
!
≤ 2
r
avec Ã
n
h
!
=
n!
h! (n −h)!
Théorème 4 : Inégalité de Hamming : Hamming inequality
Un code est parfait si et seulement si (L’inégalité de Hamming devient égalité)
Ã
n
0
!
+
Ã
n
1
!
+
Ã
n
2
!
+···Ã
n
t
!
= 2
r
Définition 5 : code parfait : Perfect code
6
UV IT4B Théorie de la communication
Un code est parfait lorsque toute erreur détectée est corrigée.
Remarque 2 :
SoitC(n,k) un code binaire et soit r = n−k le nombre de bits de contrôle. Un code de Hamming
binaire est un code 1-correcteur de longueur n = 2
r −1.
Définition 6 : Code de Hamming : Hamming code
Puisque le code de Hamming est 1-correcteur alors :
• dmin = 3
•
¡n
0
¢
+
¡n
1
¢
= 1+n = 1+2
r −1 = 2
r
, si bien que le code est parfait d’après la définition 5.
Remarque 3 :
3 Code linéaire : Linear code
Rappelons qu’un espace vectoriel de dimension k sur le corps à deux éléments est simplement l’ensemble
des mots de longueur n sur l’alphabet {0,1}, noté {0,1}
n
. Cet ensemble est muni d’une addition binaire définie par :
(x1,··· ,xn)+(y1,··· , yn) = (x1 + y1,··· ,xn + yn)
Par exemple
(01001110)+(11011100) = (10010010)
Un sous-espace vectoriel est alors un sous ensemble W de {0,1}
n
tel que
Si, x, y ∈ W alors x + y ∈ W
Un code linéaire C(n,k) sur {0,1}, est un sous-espace vectoriel de dimension k. Le nombre de
mots d’un tel code est M = 2
k
. Ainsi, puisque C(n,k) est code linéaire alors
x, y ∈C(n,k) ⇒ x + y ∈C(n,k)
Définition 7 :
7
UV IT4B Théorie de la communication
• Tout code linéaire binaire doit contenir le mot de code nul, c’est-à-dire le mot de code
avec que des 0. En effet, la somme d’un mot de code avec lui-même donne comme résultat toujours le mot nul.
• Pour un code linéaire on a dmin = wmin
Remarque 4 :
Tout sous-espace vectoriel possède une base. Un code linéaire C(n,k) est donc caractérisé par la
donnée d’une base. En effet si b1;b2,··· ;bk ∈ {0,1}
n
forment une base de C(n,k) alors tout élément
x de C(n,k) est une somme :
x =
X
k
i=1
αibi
, α ∈ {0,1}
Une matrice génératrice G d’un code linéaire C(n,k) est une matrice de dimension k × n (k
lignes et n colonnes) sur {0,1} dont les lignes forment une base de C(n,k).
Définition 8 : Matrice génératrice : Generator matrix
• Les lignes de G sont des mots du code.
• Tout mot du code est combinaison linéaire des lignes de G.
• Pour un même code il existe de nombreuses matrice génératrices. Parmi celles-ci certaines ont une forme pratique.
Remarque 5 :
Une matrice génératrice G d’un code linéaire C(n,k) est dite systématique si
G =
³
Ik , N
´
C’est-à-dire les k premières colonnes forment la matrice identité d’ordre k. Dans ce cas on dit
que le code est systématique : Systematic code.
Définition 9 : Matrice génératrice systématique: Systematic generator matrix
Exemple :
G =




1 0 0 1 1
0 1 0 1 0
| {z }
I3
0 0 1
| {z }
N
0 1




8
UV IT4B Théorie de la communication
3.1 Codage par les codes linéaires : Encoding with a Linear Code
Soit C(n,k) un code linéaire de matrice génératrices G et soit i = (i1,i2,··· ,ik ) un mot information. Alors le codage du mot information i est donné par :
c = i ×G
Définition 10 :
Dans le cas où le code est systématique on aura :
c = i ×
³
Ik , N
´
=
³
i Ik , i N´
=
³
i
|{z}
bits information
; i N
|{z}
bits de contrôle
´
Ainsi, pour un code systématique, les k premiers bits d’un mot code sont les bits d’information tandis
que les n − k bits restants constituent les bits de contrôle. Par exemple si G =
Ã
1 0 1
0 1 1
!
est matrice
génératrice 2×3 alors k = 2 et n = 3 et les mots code sont les suivants
| {z }
information







0 0
0 1
1 0
1 1







×
Ã
1 0 1
| {z }
G
0 1 1
!
=







0 0 0
0 1 1
1 0 1
| {z }
mots code
1 1 0














0
2
2
|{z}
Poids
2







Exercice : Code de contrôle de parité (Parity check code): Dans un code de parité de longueur n,
chaque mot contient k = n − 1 bits d’information plus 1 bit de parité. Le bit de parité est calculé de
telle sorte que le nombre total de 1 soit toujours pair.
Parity check code : In this code, a k-bit data word is changed to an n-bit code word where n = k +1.
The extra bit, called the parity bit, is selected to make the total number of 1 in the code word even.
1. Pour n = 3, expliciter l’ensemble des mots du code.
2. Montrer que le code est linéaire.
3. Donner sa distance minimale dmin. En déduire sa capacité de détection et de correction.
4. Donner sa matrice génératrice sous forme systématique.
5. Retrouver les mots du code en utilisant la matrice génératrice.
Solution :
9
UV IT4B Théorie de la communication
1. Pour n = 3 on a k = 2. Ainsi, le code de parité est l’ensemble des quatre mots possibles :
C = {(000), (011), (101), (110)}
2. Le code de parité est linéaire car :
• Le mot O = (000) ∈C
• La somme de deux mots de poids pair est un mot de poids pair.
3. Le code est linéaire donc dmin = wmin = 2, le code peut détecter 1 erreur mais ne pas en corriger.
4. Une base du code peut être {(101); (011}, ce qui correspond à la matrice génératrice sous forme
systématique
Ã
1 0 1
| {z }
I2
0 1
|{z}
N
1
!
5. En effectuant le produit de la matrice G par les vecteurs dont les deux composantes correspondent aux éléments binaires d’information, on obtient les mots code :
i ×G =







0 0
0 1
1 0
| {z }
i
1 1







×
Ã
1 0 1
0 1 1
!
=







0 0 0
0 1 1
1 0 1
| {z }
mots code
1 1 0














0
2
2
|{z}
Poids
2







Exercice : Construire la matrice génératrice d’un code linéaire systématique C(5,2) de distance
minimale dmin = 3.
Solution : Le code étant systématique, la première partie de la matrice génératrice est l’identité de
dimension 2. Ensuite il faut ajouter deux 1 sur chaque ligne (sinon la distance minimum ne serait pas
3). Il faut que les deux lignes ne soient pas identiques et leur somme soit de poids ≥ dmin. On obtient
donc :
G =
Ã
1 0 0 1 1
| {z }
I2
0 1
| {z }
N
1 1 0
!
Exercice : Considérons le codeC(3,2) avec une parité impaire, c’est-à-dire le bit de parité est calculé
de telle sorte que le nombre total de 1 soit toujours impair.
1. Expliciter tous les mots code.
10
UV IT4B Théorie de la communication
2. Ce code est-t-il linéaire?
Solution
1.
Mot information Mot code
00 0 0 1
01 0 1 0
10 1 0 0
11 1 1 1
2. Ce code n’est pas linéaire car le mot (000) 6∈C, en plus (111)+(100) = (011) 6∈C.
Exercice : Codes de Hamming C(7,4):
1. Construire une matrice génératrice sous forme systématique d’un code linéaire de Hamming
C(7,4).
2. Expliciter tous les mots du code.
3. Donner les relations linéaires qui lient les bits de contrôle en fonction des bits d’information.
En déduire la réalisation matérielle du codage.
Solution : On sait que les codes de Hamming ont dmin = 3.
1. Le code étant systématique, on peut écrire les quatre premières colonnes de la matriceG puisqu’on
sait que cette sous-matrice de G est l’identité de dimension 4. Ensuite il reste 3 éléments à
déterminer pour chacune des 4 lignes afin d’obtenir la matrice G.
On sait qu’alors 2 au moins des trois éléments à déterminer doivent prendre la valeur 1 pour
faire en sorte qu’il n’y ait pas de vecteurs de base, donc de mots code, qui aient un poids inférieur strictement à 3, sinon la distance minimum du code ne serait pas 3. De plus, il faut faire
en sorte que les éléments ajoutés sur deux lignes différentes ne coïncident pas, sinon le mot
code obtenu en effectuant la somme des deux vecteurs correspondant à ces lignes aurait un
poids égal à 2, ce qui serait en contradiction avec la valeur 3 de la distance minimum. Compte
tenu de ces contraintes, on peut choisir pour G la matrice génératrice sous forme systématique
suivante:
G =







1 0 0 0 1 0 1
0 1 0 0 1 1 0
0 0 1 0 1 1 1
| {z }
I4
0 0 0 1
| {z }
N
0 1 1







11
UV IT4B Théorie de la communication
2. En multipliant G par les 2
4 mots d’information possibles, on obtient :







































0 0 0 0
0 1 0 0
1 0 0 0
1 1 0 0
0 0 1 0
0 1 1 0
1 0 1 0
1 1 1 0
0 0 0 1
0 1 0 1
1 0 0 1
1 1 0 1
0 0 1 1
0 1 1 1
1 0 1 1
| {z }
Mots information
1 1 1 1







































×







1 0 0 0 1 0 1
0 1 0 0 1 1 0
0 0 1 0 1 1 1
| {z }
I4
0 0 0 1
| {z }
N
0 1 1







=







































0 0 0 0 0 0 0
0 1 0 0 1 1 0
1 0 0 0 1 0 1
1 1 0 0 0 1 1
0 0 1 0 1 1 1
0 1 1 0 0 0 1
1 0 1 0 0 1 0
1 1 1 0 1 0 0
0 0 0 1 0 1 1
0 1 0 1 1 0 1
1 0 0 1 1 1 0
1 1 0 1 0 0 0
0 0 1 1 1 0 0
0 1 1 1 0 1 0
1 0 1 1 0 0 1
| {z }
Mots code
1 1 1 1 1 1 1














































































0
3
3
4
4
3
3
4
3
4
4
3
3
4
4
|{z}
Poids
7







































3. Posons i = (i1,i2,i3,i4) un mot information. Alors le codage du mot information i est donné
par :
c = i ×
³
Ik , N
´
=
³
i I4, i N´
=
³
i
|{z}
bits information
; i N
|{z}
bits de contrôle
´
Posons (c1,c2,c3) les bits de contrôle. Il s’ensuit que
(c1,c2,c3) = i N = (i1,i2,i3,i4)







1 0 1
1 1 0
1 1 1
0 1 1







= (i1 +i2 +i3, i2 +i3 +i4, i1 +i3 +i4)
On en déduit les relations linéaires du code
c1 = i1 +i2 +i3
c2 = i2 +i3 +i4
c3 = i1 +i3 +i4
Voici la réalisation matérielle du codage
12
UV IT4B Théorie de la communication
i1 i2 i3 i4
c2 = i2 +i3 +i4
c1 = i1 +i2 +i3
c3 = i1 +i3 +i4
13
UV IT4B Théorie de la communication
3.2 Décodage des codes linéaires
Une matrice H de taille (n −k)×n d’un code linéaire C(n,k) est dite de contrôle ou de test ssi
pour tout mot x du code on a
H xt = 0
où x
t
est le transposé de x, c’est-à-dire si x = (x1,x2,··· ,xn) alors
x
t =







x1
x2
.
.
.
xn







Définition 11 : Matrice de contrôle : Parity check matrix
– Pour la matrice génératrice sous forme systématique G =
³
Ik , N
´
, la matrice H est
donnée par :
H =
³
N
t
, In−k
´
où N
t
est la transposée de la matrice N.
– La distance minimale dmin est égal au nombre minimal de colonnes linéairement
dépendantes dans H, autrement dit tout ensemble de dmin − 1 colonnes de H doit
être linéairement indépendant.
Propriétés 1 :
Exemple : Si G =
Ã
1 0 1 1 1
| {z }
I2
0 1
| {z }
N
0 1 1
!
, avec I2 =
Ã
1 0
0 1
!
, N =
Ã
1 1 1
0 1 1
!
Alors
H =




1 0 1 0 0
1 1 0 1 0
| {z }
N
t
1 1
| {z }
I3
0 0 1




Calcul de dmin
• dmin 6= 1 car il n’y a pas de colonne de zéros dans H,
• dmin 6= 2 car il n’y a pas deux colonnes identiques dans H,
• dmin = 3 car la somme de la première colonne et la deuxième donne la troisième colonne. Par
conséquent le nombre minimal de colonnes linéairement dépendantes dans H est 3.
14
UV IT4B Théorie de la communication
3.2.1 Opération de décodage:
A partir de la matrice de contrôle on peut définir l’algorithme de décodage suivant :
Pour contrôler l’appartenance au code d’un mot reçu y, il suffit d’effectuer le produit :
H yt = S
• Si S = 0 alors y appartient au code,
• Si S 6= 0 le mot reçu y a subi des altérations.
On appelle syndrome de y le vecteur
S(y) = H yt
avec y un mot reçu. l’ordre de S(y) est r = n −k.
Définition 12 :
Propriété du syndrome : Soit x un mot émis. Supposons que pendant la transmission, certains bits
de x soient changés. Dans ce cas, on peut exprimer le mot reçu y comme
y = x +e
Le mot e (erreur) contient alors des "1" à l’emplacement des bits erronés, les autres étant égaux à
0. Par exemple, si x =




0
1
1




et que 2 erreurs se produisent, l’une en première position et l’autre en
deuxième position, le mots reçu est donc y =




1
0
1




. Ainsi
y =




1
0
1



 =




0
1
1




|{z}
x
+




1
1
0




|{z}
e
Ainsi, puisque x est un mot code alors H xt = 0. Il s’ensuit
S(y) = H yt = H(x
t +e
t
) = H xt
|{z}
=0
+Het = Het
Notons h1,h2,··· ,hn les colonnes de H et e = (e1,e2,··· ,en). Alors, Het
implique
S(y) = h1e1 +h2e2 +··· +hnen
15
UV IT4B Théorie de la communication
Puisque les ei ne peuvent prendre que 0 ou 1, alors le syndrome S est la somme des colonnes de H
correspondant aux bits erronés (ei = 1). Nous pouvons alors exploiter ce résultat dans le cas où l’on
est sûr que le mot reçu y ne contient pas plus d’une erreur.
S’il y a au plus une erreur dans le mot reçu, alors le syndrome S recopiera la colonne de H qui
correspond à l’endroit du bit erroné.
Propriété 2 :
Exercice : Soit C le code engendré par la matrice
G =







1 0 0 0 1 1
0 1 0 0 1 1
0 0 1 0 1 1
| {z }
I4
0 0 0 1
| {z }
N
0 1







1. Déterminer le nombre de mots de code.
2. Déterminer la matrice de contrôle H.
3. Calculer la distance minimale du code.
4. En déduire le nombre d’erreurs que le code peut détecter/corriger.
Solution
1. La dimension est k = 4 (nombre de lignes de G), donc il y a 2
4 = 16 mots code.
2. H =
Ã
1 1 1 0 1 0
1 1 1 1 0 1
!
3. La distance minimale est dmin = 2 car il y a des colonnes identiques dans H.
4. Puisque dmin = 2 > 1 le code peut détecter une erreur mais ne pas en corriger.
Exercice : Soit C le code engendré par les équations suivantes :
c1 = i1 +i2 +i4
c2 = i1 +i3
c3 = i2 +i3 +i4
c4 = i2
où (i1,i2,i3,i4) sont les bits d’informations et (c1,c2,c3,c4) sont les bits de contrôle. Un mot de code
s’écrit donc (i1,i2,i3,i4,c1,c2,c3,c4).
16
UV IT4B Théorie de la communication
1. Déterminer les paramètres n et k du code. En déduire le nombre de mots de code.
2. Déterminer une matrice génératrice G de C sous forme systématique :
3. En déduire la matrice de contrôle H.
4. Calculer la distance minimale du code.
5. En déduire le nombre d’erreurs que le code peut détecter/corriger.
6. Supposons que le mot reçu soit Y = 10110110. Ce mot peut-il être le résultat d’une transmission comportant une erreur ? Si oui, décoder Y et déterminer le mot source dont il provient.
Même question si on suppose qu’il y a deux erreurs de transmission.
7. Peut-on corriger Y si on sait à priori que les erreurs portent sur 2 bits consécutifs.
Solution
1. Longueur du mot de code n = 8, longueur du mot d’information k = 4. Donc il y a 2
4 = 16 mots
code.
2. D’après les équations qui engendrent le code on trouve
G =







1 0 0 0 1 1 0 0
0 1 0 0 1 0 1 1
0 0 1 0 0 1 1 0
| {z }
I4
0 0 0 1
| {z }
N
1 0 1 0







3.
H =







1 1 0 1 1 0 0 0
1 0 1 0 0 1 0 0
0 1 1 1 0 0 1 0
| {z }
N
t
0 1 0 0
| {z }
I4
0 0 0 1







4. Les colonnes de H sont 2 à 2 distinctes, ce qui assure que dmin ≥ 3. On remarque de plus que
Col onne1 +Col onne5 +Col onne6 = 0, donc dmin = 3.
5. Puisque 3 > 2 et 3 > 2×1 alors le code peut détecter toutes les configurations de 1 et 2 erreurs
et peut corriger toutes les configurations de 1 erreur.
6. Le syndrome de y est S(y) = H yt =







0
1
1
0







6= 0. Donc y n’est pas un mot de code. Une et une seule
17
UV IT4B Théorie de la communication
colonne de H est égale à







0
1
1
0







, c’est la troisième colonne. Cela signifie qu’il y a un unique mot
erreur de poids 1 de même syndrome que y. Il s’agit de e =
³
0 0 1 0 0 0 0 0
´
. Si l’on
sait qu’il y a une seule erreur de transmission on peut donc corriger y en x = y +e = 10010110.
Si l’on suppose maintenant que l’on a 2 erreurs de transmission, la capacité de correction ne
nous permet ni d’affirmer que l’on sait décoder, ni que l’on ne sait pas. On cherche s’il existe
des mots de poids 2 et de même syndrome que y. Or, on remarque que S(y) = Col onne1 +
Col onne4 = Col onne6 +Col onne7, donc il y a deux erreurs possibles. La première en positions 1 et 4, la deuxième en position 6 et 7. Ainsi, les vecteurs erreurs correspondants sont
e1 = (10010000) et e2 = (00000110). On ne sait donc pas corriger y.
7. Si l’on suppose que les erreurs portent sur des bits consécutifs alors on peut affirmer que les
erreurs de transmission sont e2 = (00000110) si bien que le mot émis est
x = y +e2 = (10110000)
4 Décodage par tableau standard
Nous allons à présent exposer une règle de décodage à distance minimale reposant sur un tableau,
dit tableau standard. En effet, Soit y un message reçu erroné, donc y = x +e avec x le message émis
et e le vecteur d’erreur. Remarquons que la relation y = x +e implique
x = y +e
Ainsi, pour construire x à partir du message reçu y, l’idée est d’ajouter à y le plus probable vecteur
d’erreur e. Nous l’appellerons le vecteur de correction ².
Une étude comparative des probabilités d’apparition des erreurs en fonction de leur poids conduit
au résultat suivant :
Soit y un message reçu de longueur n avec une probabilité d’erreur par bit p. Sous la condition
np < 1 on a :
Les erreurs de poids le plus faible sont les plus probables.
Théorème 5 :
A partir de ce résultat, le vecteur de correction ² doit répondre aux exigences suivantes :
1. Être tel que y +² soit un mot de code.
2. Être de poids le plus faible possible.
18
UV IT4B Théorie de la communication
Question : Comment chercher le vecteur de correction ²?
Réponse : Dire que y +² est un mot de code signifie que son syndrome est égal 0, c’est-à-dire
S = H(y
t +²
t
) = 0
Ceci implique
H yt + H²
t = 0
Ce qui entraîne
H yt = H²
t ⇒ S(y) = S(²)
On en déduit que le syndrome de y est le même que celui de ².
Ainsi, pour chercher le vecteur de correction ² parmi tous les mots de longueur n, il suffit de chercher
les vecteurs du même syndrome que le message reçu y et de choisir celui dont le poids est le plus
faible. D’où la définition suivante :
2 mots de longueur n seront équivalents s’ils ont le même syndrome.
Définition 13 :
A partir de cette définition, les mots de longueur n sont alors répartis en classes par syndrome. Ainsi,
2 mots appartiennent à la même classe s’ils ont le même syndrome.
• Le nombre de classes (syndrome) est 2
n−k
.
• Le nombre de mots par classe est 2
k
.
• le syndrome (00···0) est celui des mots code.
Remarque 6 :
Question : Comment construire les classe?
Réponse : Pour construire toutes les classes, on applique les étapes suivantes :
• La classe 0 est réservée aux mots code c1,c2,c3,··· de syndrome S0 = (00···0).
Syndromes S0 S1 S2 ······
Classe 0 Classe 1 Classe 2 ······
c1
c2
c3
.
.
.
Vecteurs de correction ²0 = (00···0)
19
UV IT4B Théorie de la communication
• Pour construire la classe 1, on choisit arbitrairement un mots y1 6∈ C(n,k). Ensuite, on calcule
son syndrome par H yt
1
= S1. Les autres mots de même syndrome S1 sont calculés en ajoutant
les mots codes à y1 :
y1 +c1; y1 +c2; y1 +c3,···
Dans cette classe, un mots de poids minimum (il peut y en avoir plusieurs) est pris comme
vecteur de correction ²1.
Syndromes S0 S1 S2 ······
Classe 0 Classe 1 Classe 2 ······
c1 y1 +c1
c2 y1 +c2
c3 y1 +c3
.
.
.
.
.
.
Vecteurs de correction ²0 = (00···0) ²1
• Pour construire la classe 2, on choisit arbitrairement un mots y2 6= y1 6∈ C(n,k). Ensuite, on
calcule son syndrome par H yt
2
= S2. Les autres mots de même syndrome S2 sont alors calculés
comme précédemment :
y2 +c1; y2 +c2; y2 +c3,···
Syndromes S0 S1 S2 ······
Classe 0 Classe 1 Classe 2 ······
c1 y1 +c1 y2 +c1
c2 y1 +c2 y2 +c2
c3 y1 +c3 y2 +c3
.
.
.
.
.
.
.
.
.
Vecteurs de correction ²0 = (00···0) ²1 ²2
• Le processus est itéré jusqu’à ce que tous les mots de longueur n soient représentés dans le
tableau.
Problème : Pour n grand, cette méthode n’est pas convenable car il faut stocker 2
n mots plus 2
n−k
syndromes, ce qui pose le problème de capacité de la mémoire.
Solution : Au lieur de stocker les 2
n mots on stocke que les 2
n−k
syndromes et les vecteurs de correction ²j
. Ces derniers s’appellent dans ce cas les représentants. On établit ainsi une table réduite
de correction à deux colonnes, et sur chaque ligne un représentant d’une classe et son syndrome:
20
UV IT4B Théorie de la communication
Vecteurs de correction Syndromes
²0 S0
²1 S1
²2 S2
.
.
.
.
.
.
Conclusion : Algorithme de décodage: A l’arrivée d’un message y, son syndrome est calculé par
S(y) = H yt
, ce qui permet de connaître son vecteur de correction ²j
. Le message est alors corrigé par
y +²j
.
Exemple : Soit le code linéaire C(4,2) de matrice génératrice et de contrôle
G =
Ã
1 0 1 0
0 1 1 1
!
; H =
Ã
1 1 1 0
0 1 0 1
!
On a k = 2, n = 4 et dmin = 2 (car deux colonnes identiques dans H). Donc ce code peut détecter 1
erreurs mais ne peut pas corriger toutes les configurations d’une erreur.
– En multipliant G par les quatre mots d’information possibles, on obtient :







0 0
0 1
1 0
| {z }
Information
1 1







×
Ã
1 0 1 0
| {z }
G
0 1 1 1
!
=







0 0 0 0 = c1
0 1 1 1 = c2
1 0 1 0 = c3
| {z }
Mots code
1 1 0 1 = c4







– Il ya 2
4 = 16 mots possibles de longueur n = 4 qui se répartissent en 2
2 = 4 classes suivant leurs
syndrome.
– La taille des syndromes est r = n −k = 2, Ainsi les syndromes possibles sont
{(00); (01); (10); (11)}
– La classe 0 est réservée aux mots code de syndrome S0 = (00).
Syndromes S0 = (00) S1 S2 ······
Classe 0 Classe 1 Classe 2 ······
(0000) = c1
(0111) = c2
(1010) = c3
(1101) = c4
Vecteurs de correction ²0 = (0000)
– Construction de la classe 1 : on choisit arbitrairement un mots y1 = (0001) 6∈ C(n,k). Ensuite, on
21
UV IT4B Théorie de la communication
calcule son syndrome par H yt
1
=
Ã
0
1
!
= S1. Les autres mots de même syndrome S1 sont calculés
comme:
y1 +c1 = (0001); y1 +c2 = (0110); y1 +c3 = (1011); y1 +c4 = (1100)
Le vecteur de correction de poids minimum est alors ²1 = (0001).
Syndromes S0 S1 = (01) S2 ······
Classe 0 Classe 1 Classe 2 ······
(0000) (0001)
(0111) (0110)
(1010) (1011)
(1101) (1100)
Vecteurs de correction ²0 = (0000) ²1 = (0001)
– Construction de la classe 2 : on choisit arbitrairement un mots y2 = (0010) 6= y1 6∈ C(n,k). Ensuite,
on calcule son syndrome par H yt
2
=
Ã
1
0
!
= S2. Les autres mots de même syndrome S2 sont alors
calculés comme précédemment :
y2 +c1 = (0010); y2 +c2 = (0101); y2 +c3 = (1000); y2 +c4 = (1111)
Les vecteurs de correction de poids minimum sont alors ²2 = (0010) ou ²2 = (1000) (Pourquoi?) (car
ce code ne permet pas de corriger toutes les configurations d’une erreur dmin = 2)
Syndromes S0 S1 = (01) S2 = (10) ······
Classe 0 Classe 1 Classe 2 ······
(0000) (0001) (0010)
(0111) (0110) (0101)
(1010) (1011) (1000)
(1101) (1100) (1111)
Vecteurs de correction ²0 = (0000) ²1 = (0001) ²2 = (0010) ou ²2 = (1000)
– Construction de la classe 3 : En suivant la même méthode pour y3 = (0100) en trouve
Syndromes S0 S1 = (01) S2 = (10) S3 = (11)
Classe 0 Classe 1 Classe 2 Classe 3
(0000) (0001) (0010) (0100)
(0111) (0110) (0101) (0011)
(1010) (1011) (1000) (1110)
(1101) (1100) (1111) (1001)
Vecteurs de correction ²0 = (0000) ²1 = (0001) ²2 = (0010) ou ²2 = (1000) ²3 = (0100)
Finalement, la table réduite de correction à deux colonnes est donnée par :
22
UV IT4B Théorie de la communication
Vecteurs de correction Syndromes
²0 = (0000) S0 = (00)
²1 = (0001) S1 = (01)
²2 = (0010) ou ²2 = (1000) S2 = (10) : Pas de correction
²3 = (0100) S3 = (11)
Conclusion : L’utilisation de la table de décodage se traduira par une décision correcte sur un mot
code si aucune erreur ne s’est produite (les mots code) ou si l’une des configurations d’une erreur en
positon 2 correspondent à la classe 3 ou en positon 4 correspondent à la classe 1. L’erreur en position
1 et 3 ne peuvent pas être corrigées puisque pour cette classe deux vecteurs de correction de même
poids sont possibles.
Correction d’un mot reçu : Supposons que le mot reçu soit y = (1110). On calcule d’abord son
syndrome S = H yt =
Ã
1 1 1 0
0 1 0 1
!







1
1
1
0







=
Ã
1
1
!
= S3. D’après la table réduite, le vecteur de correction
est alors ²3 = (0100). Le mot reçu y est alors corrigé par :
y +²3 = (1110)+(0100) = (1010) = mot émis
23
UV IT4B Théorie de la communication
TD
Exercice 1 : Un code associe à deux éléments binaires d’information des mots constitués de cinq
éléments binaires suivant la correspondance:
1. Montrer que le code est systématique et déterminer sa matrice génératrice. En déduire sa matrice de contrôle.
2. Calculer la dmin du code. En déduire sa capacité de correction.
3. Construire la table de décodage répondant au principe du décodage à distance minimum.
Solution :
1. On constate que les deux premiers éléments binaires des mots code correspondent aux éléments binaires d’information. Donc le code est systématique.
Si on note (x1,x2,x3,x4,x5), les mots code (x1,x2,x3 représentent les éléments binaires d’information
et x4,x5 les éléments binaires de contrôle), on a d’après les mots code
x4 = x1 + x2
x5 = x3
La matrice génératrice sous forme systématique s’écrit alors
G =




1 0 0 1 0
0 1 0 1 0
| {z }
I3
0 0 1
| {z }
N
0 1




On en déduit la matrice de contrôle :
H = (N
t
,I3) =
Ã
1 1 0 1 0
0 0 1 0 1
!
24
UV IT4B Théorie de la communication
2. D’après les mots code, on a dmin = 2. Le code est donc 0-correcteur.
3. – Il y a 2
5 = 32 mots possibles de longueur n = 5 qui se répartissent en 2
2 = 4 classes suivant
leurs syndrome.
– La taille des syndromes est r = n −k = 2, Ainsi les syndromes possibles sont
{(00); (01); (10); (11)}
– La classe 0 est réservée aux mots code de syndrome S0 = (00).
Syndromes S0 = (00) S1 S2 ······
Classe 0 Classe 1 Classe 2 ······
(00000) = c1
(00101) = c2
(01010) = c3
(01111) = c4
(10010) = c5
(10111) = c6
(11000) = c7
(11101) = c8
Vecteurs de correction ²0 = (0000)
– Construction de la classe 1 : on choisit arbitrairement un mots y1 = (00001) 6∈ C(n,k). Ensuite, on calcule son syndrome par H yt
1
=
Ã
0
1
!
= S1. Les autres mots de même syndrome S1
sont calculés comme:
y1 +c1 = (00001); y1 +c2 = (00100);······ ; y1 +c8 = (11100)
Les vecteur de correction de poids minimum sont alors ²1 = (00001) ou ²2 = (00100).
Syndromes S0 = (00) S1 = (01) S2 ······
Classe 0 Classe 1 Classe 2 ······
(00000) = c1 (00001)
(00101) = c2 (00100)
(01010) = c3 (01011)
(01111) = c4 (01110)
(10010) = c5 (10011)
(10111) = c6 (10110)
(11000) = c7 (11001)
(11101) = c8 (11100)
Vecteurs de correction ²0 = (0000) ²1 ou ²2
25
UV IT4B Théorie de la communication
– Construction de la classe 2 : on choisit arbitrairement un mots y2 = (00010) 6= y1 6∈ C(n,k).
Ensuite, on calcule son syndrome par H yt
2
=
Ã
1
0
!
= S2. Les autres mots de même syndrome S2
sont alors calculés comme précédemment :
y2 +c1 = (00010); y2 +c2 = (00111);······ ; y2 +c4 = (11111)
Les vecteurs de correction de poids minimum sont alors ²3 = (00010) ou ²4 = (01000) ou ²5 =
(10000)
Syndromes S0 = (00) S1 = (01) S2 = (10) ······
Classe 0 Classe 1 Classe 2 ······
(00000) = c1 (00001) (00010)
(00101) = c2 (00100) (00111)
(01010) = c3 (01011) (01000)
(01111) = c4 (01110) (01101)
(10010) = c5 (10011) (10000)
(10111) = c6 (10110) (10101)
(11000) = c7 (11001) (11010)
(11101) = c8 (11100) (11111)
Vect-correction ²0 = (0000) ²1 ou ²2 ²3 ou ²4 ou ²5
– Construction de la classe 3 : En suivant la même méthode pour y3 = (01100) en trouve
Syndromes S0 = (00) S1 = (01) S2 = (10) S1 = (11)
Classe 0 Classe 1 Classe 2 Classe 3
(00000) = c1 (00001) (00010) (01100)
(00101) = c2 (00100) (00111) (01101)
(01010) = c3 (01011) (01000) (00110)
(01111) = c4 (01110) (01101) (00011)
(10010) = c5 (10011) (10000) (11110)
(10111) = c6 (10110) (10101) (11011)
(11000) = c7 (11001) (11010) (10100)
(11101) = c8 (11100) (11111) (10001)
Vect-correction ²0 = (0000) ²1 ou ²2 ²1 ou ²3 ou ²5 ²1 ou ²3 ou ²4 ou ²5 ou ²6
Finalement, la table réduite de correction à deux colonnes est donnée par :
Vecteurs de correction Syndromes
²0 = (0000) S0 = (00)
Pas de correction S1 = (01) :
Pas de correction S2 = (10) :
Pas de correction S3 = (11) :
26
UV IT4B Théorie de la communication
Conclusion : L’utilisation de la table de décodage se traduira par une décision correcte sur un
mot code si aucune erreur ne s’est produite (les mots code). Aucune configuration d’une erreur
ne peut être corrigée car dmin = 2.
Exercices 2 : Soit C(6,3) le code avec matrice de contrôle :
H =




1 1 0 1 0 0
0 1 1 0 1 0
1 0 1 0 0 1




1. Donner les relations linéaires qui lient les bits de contrôle en fonction des bits d’information.
En déduire la réalisation matérielle du codage et décodage.
2. Déterminer la matrice génératrice G.
3. Calculer dmin du code. En déduire sa capacité de correction.
4. Construire la table réduite de décodage répondant au principe du décodage à distance minimum.
5. Corriger les mots reçu suivants y = (100110) et (011101).
Solution :
1. Soit x = (i1,i2,i3,c1,c2,c3) un mot code. (i1,i2,i3 sont les bits d’information, c1,c2,c3 sont les
bits de contrôle). Puisque x est un mot code, la condition H xt = 0 permet d’écrire le système
suivant :



i1 +i2 +c1 = 0
i2 +i3 +c2 = 0
i1 +i3 +c3 = 0
Il s’ensuit alors



c1 = i1 +i2
c2 = i2 +i3
c3 = i1 +i3
Voici la réalisation matérielle du codage :
27
UV IT4B Théorie de la communication
i1 i2 i3
c2 = i2 +i3
c1 = i1 +i2
c3 = i1 +i3
Réalisation matérielle du décodage : Soit y = (y1, y2, y3, y4, y5, y6) un mot reçu. Son syndrome
est donné par
S =




s1
s2
s3



 = H yt =




1 1 0 1 0 0
0 1 1 0 1 0
1 0 1 0 0 1
















y1
y2
y3
y4
y5
y6












=




y1 + y2 + y4 = s1
y2 + y3 + y5 = s2
y1 + y3 + y6 = s3




y1 y2 y3 y4 y5 y6
s2 = y2 + y3 + y5
s1 = y1 + y2 + y4
s3 = y1 + y3 + y6
28
UV IT4B Théorie de la communication
2.
G =




1 0 0 1 0 1
0 1 0 1 1 0
0 0 1 0 1 1




3. dmin = 3. Le code est donc 1,2-détecteur et 1-correcteur.
4. La table de décodage répondant au principe du décodage à distance minimum est :
Vecteurs de correction Syndromes
²1 = (000000) S1 = (000)
²2 = (100000) S2 = (101)
²3 = (010000) S3 = (110)
²4 = (001000) S4 = (011)
²5 = (000100) S5 = (100)
²6 = (000010) S6 = (010)
²7 = (000001) S7 = (001)
²8 = (100010) S8 = (111)
5. • On calcule son syndrome S = H yt =




1 1 0 1 0 0
0 1 1 0 1 0
1 0 1 0 0 1
















1
0
0
1
1
0












=




0
1
1



 = S4. D’après la table
réduite, le vecteur de correction est alors ²4 = (001000). Le mot reçu y est alors corrigé par
:
y +²4 = (100110)+(001000) = (101110) = mot émis
• On calcule son syndrome S = H yt =




1 1 0 1 0 0
0 1 1 0 1 0
1 0 1 0 0 1
















0
1
1
1
0
1












=




0
0
0



 = S1. Le mot reçu y
est un mot code.
Exercice 4 : Un code associe à deux éléments binaires d’information des mots constitués de cinq
éléments binaires suivant la correspondance :
29
UV IT4B Théorie de la communication
Mots information Mots code
00 00000
01 01101
10 10111
11 11010
1. Montrer que le code est systématique et déterminer sa matrice génératrice. En déduire sa matrice de contrôle.
2. Calculer la dmin du code. En déduire sa capacité de correction.
3. Construire la table réduite de décodage répondant au principe du décodage à distance minimum.
4. Corriger le mot reçu suivant y = (11101).
Solution :
1. On constate que les deux premiers éléments binaires des mots code correspondent aux éléments binaires d’information. Donc le code est systématique.
Si on note (x1,x2,x3,x4,x5), les mots code (x1,x2 représentent les éléments binaires d’information
et x3,x4,x5 les éléments binaires de contrôle), on a d’après les mots code
x3 = x1 + x2
x4 = x1
x5 = x1 + x2
La matrice génératrice sous forme systématique s’écrit alors
G =
Ã
1 0 1 1 1
| {z }
I2
0 1
| {z }
N
1 0 1
!
On en déduit la matrice de contrôle :
H = (N
t
,I3) =




1 1 1 0 0
1 0 0 1 0
1 1 0 0 1




2. D’après les mots code, on a dmin = 3. Le code est donc 1-correcteur.
3. Pour construire la table de décodage, il faut recenser tous les syndromes (classes) possibles. En
effet, on a n = 5, k = 2. Ainsi, le nombre de syndromes est 2
5−2 = 8. Le nombre de mots par
classe est 2
k = 2
2 = 4.
30
UV IT4B Théorie de la communication
• Le syndrome de ²0 = 00000 est S0 = 000 correspondent aux mots code.
• Puisque le code est 1-correcteur, on cherche les syndromes des configurations d’une erreur (Il y a 5 configurations d’une erreur qui peuvent être corrigées). Inutile de chercher
les mots de chaque classe.
Vecteurs de correction Syndromes
²1 = (00001) S1 = (001) = H²
t
1
²2 = (00010) S2 = (010)
²3 = (00100) S3 = (100)
²4 = (01000) S4 = (101)
²5 = (10000) S5 = (111)
4. On calcule son syndrome S = H yt =




1 1 1 0 0
1 0 0 1 0
1 1 0 0 1













1
1
1
0
1









=




1
1
1



 = S5. D’après la table réduite,
le vecteur de correction est alors ²5 = (01000). Le mot reçu y est alors corrigé par :
y +²5 = (11101)+(10000) = (01101) = mot émis
Exercice 3 : Soit C(4,2) un code linéaire dont la matrice génératrice est donnée par
G =
Ã
1 0 1 0
| {z }
I2
0 1
| {z }
N
1 1
!
1. Donner les mots code.
2. Déterminer la matrice de contrôle H.
3. Calculer dmin du code. En déduire sa capacité de correction.
4. Construire la table de décodage répondant au principe du décodage à distance minimum.
5. Corriger les mots reçu suivants y = (1100) et (0101).
Solution :
1. Les mots code sont :
31
UV IT4B Théorie de la communication
mot d’info mot code
00 0000
01 0111
10 1010
11 1101
2. La matrice de contrôle est donnée par :
H = (N
t
,I2) =
Ã
1 1 1 0
0 1 0 1
!
3. D’après les mots code, on a dmin = 2. Le code est donc 1-détecteur et 0-correcteur.
4. Pour construire la table de décodage, il faut recenser tous les syndromes (classes) possibles. En
effet, on a n = 4, k = 2. Ainsi, le nombre de syndromes est 2
4−2 = 4. Le nombre de mots par
classe est 2
k = 2
2 = 4.
• Le syndrome de ²0 = 0000 est S0 = 00 correspondent aux mots code.
• Puisque le code est 0-correcteur, on cherche les syndromes des configurations d’une erreur qui peuvent être corrigées.
Vecteurs de correction Syndromes
²1 = (0000) S1 = (00)
²2 = (0001) S2 = (01)
²3 = (1000) ou (0010) S3 = (10)
²4 = (0100) S4 = (11)
5. • On calcule son syndrome S = H yt =
Ã
1 1 1 0
0 1 0 1
!







1
1
0
0







=
Ã
0
1
!
= S2. D’après la table réduite, le vecteur de correction est alors ²5 = (0001). Le mot reçu y est alors corrigé par
:
y +²5 = (1100)+(0001) = (1101) = mot émis
• On calcule son syndrome S = H yt =
Ã
1 1 1 0
0 1 0 1
!







0
1
0
1







=
Ã
1
0
!
= S3. D’après la table réduite, le message erroné (0101) n’est pas corrigible.
32
UV IT4B Théorie de la communication
Exercice 5 : Soit C un code de distance minimale 5 et soit H une matrice de contrôle. Est-il possible
d’avoir H(111000)t = H(001110)t
?
Solution : On réécrit H(111000)t = H(001110)t
en faisant tout passer à gauche
H(111000)t + H(001110)t = O
puis en factorisant par H
H
h
(111000)t +(001110)t
i
= O
ce qui donne
H(110110)t = 0
ce qui signifie que le mot (110110) est dans le Code C. Or ce mot est de poids 4 et le poids minimum
de C est 5, ce qui n’est pas possible.
33
UV IT4B Théorie de la communication
Final 2023 : Considérons une source sans mémoire S = {A,B} avec p(A) = 0,98 et p(B) = 0,02. Le
débit symboles de la source est DS = 300 K s ymbol es/sec. En outre, on dispose d’un canal binaire
symétrique de probabilité d’erreur p = 0,05 fonctionnant au débit d’utilisation de DC = 280 kbi t s/sec.
Dans les calculs demandés, on prendra que les 4 chiffres après la virgule
Question préliminaire
Peut-on envisager d’utiliser ce canal pour transmettre le contenu de la source S ? (2 pts)
Partie A: Codage de source
1) On se propose de réduire le débit binaire de la source S d’au moins 50% grâce à un code de
Huffman. Quel doit être l’ordre minimum d’extension de la source S permettant d’assurer une
telle performance ? (1 pt)
2) Construire le codage de Huffman de l’extension d’ordre 3 de la source S en utilisant le modèle
du tableau suivant : (2 pts)
4! On veillera à respecter scrupuleusement la convention suivante : On associe le bit 1 à
chaque embranchement partant vers la droite et le bit 0 vers la gauche.
Mots source d’ordre 3 AAA AAB ABA ABB BAA BAB BBA BBB
Notation des mots s0 s1 s2 s3 s4 s5 s6 s7
Codage Huffman ? ? ? ? ? ? ? ?
3) Soit S
0
la nouvelle source binaire obtenue à partir de S par le codage de Huffman. Calculer le
taux de réduction du débit binaire effectif par le codage de Huffman. (2 pts)
4) Quel est le nombre de bits de contrôle que l’on doit ajouter à deux bit d’information si on veut
utiliser le canal à son débit nominal de 280 kbi t s/sec? (4 pts)
Partie B: Codage du canal
On se propose de coder l’information binaire obtenue après le codage de Huffman de la question 1
en utilisant le code C(5,2) engendré par les équations suivantes :
c1 = i1
c2 = i2
c3 = i1 +i2
où (i1,i2) sont les bits d’informations et (c1,c2,c3) sont les bits de contrôle. Un mot de code s’écrit
donc (i1,i2,c1,c2,c3).
5) (a) Expliciter tous les mots code. (2 pts)
(b) Calculer la distance minimale du code. (2 pts)
34
UV IT4B Théorie de la communication
(c) En déduire le nombre d’erreurs que le code peut détecter/corriger. (1 pt)
(d) Déterminer une matrice génératrice G de C(5,2) sous forme systématique. (2 pts)
(e) Déterminer la matrice de contrôle H. (1 pt)
(f ) La table de décodage indiquant les configurations d’erreurs effectivement corrigées de ce
code est donnée ci-dessous. Corriger le mot reçu suivants 11010. (2 pts)
Syndromes Vecteurs de correction
000 00000
001 00001
010 00010
011 01000
100 00100
101 10000
110 00110
111 10010
35
UV IT4B Théorie de la communication
Solution
Question préliminaire
Il faut vérifier que la condition du deuxième théorème de Shannon est satisfaite.
– L’entropie de la source S par symbole est: H(S) = H2(0,98) = 0,1414 bi t.
– La capacité du canal par utilisation est C = 1− H2(0,05) = 0,7136 bi t.
– Débit d’entropie de S: H0 = 300 103 ×0,1414 = 42420 bi t s/sec.
– Capacité du canal par unité de temps: C
0 = 0,7136×280 103 = 199808 bi t s/sec.
On vérifie H0 < C
0
, on peut donc théoriquement transmettre le contenu de la source sur le canal.
Partie A: Codage de source
1) La source étant sans mémoire, si M est l’ordre d’extension, on sait qu’il existe un code préfixe dont le nombre moyen d’éléments binaires n utilisés pour représenter un symbole source
vérifie;
H(S) ≤ n ≤ H(S)+
1
M
Si on veut obtenir une réduction du débit initial d’au moins 50%, il ne faut pas que n dépasse
0,5 puisqu’à un symbole source, il correspond (en moyenne) n éléments binaires code. M doit
donc vérifier: 0,141 +
1
M
< 0,5 soit 1
M
< 0,359, c’est-à-dire M > 2,78. Il faut donc prendre
l’extension d’ordre 3.
2) Calculons les probabilités des huit mots de l’extension d’ordre 3 de S
désignation des mots mots source d’ordre 3 probabilités
s0 AAA (0,98)3 = 0,941
s1 AAB (0,98)2 ×0,02 = 0,019
s2 ABA 0,019
s3 ABB 0,98×(0,02)2 = 3,92 10−4
s4 BAA 0,019
s5 BAB 3,92 10−4
s6 BBA 3,92 10−4
s7 BBB 8 10−6
36
UV IT4B Théorie de la communication
Voir tableau.
Mots source d’ordre 3 AAA AAB ABA ABB BAA BAB BBA BBB
Notation des mots s0 s1 s2 s3 s4 s5 s6 s7
Codage Huffman 1 011 010 00011 001 00010 00001 00000
3) La longueur moyenne d’un mot code (correspondant à un mot source de longueur 3) est;
n3 = 5×8 10−4 +5×3×3,92 10−4 +3×3×0,019+1×0,941 u 1,118 bi t s
Donc, à un symbole source, il correspond n =
n3
3
=
1,118
3
= 0,3726 élément binaire code. Le
débit binaire DS
0 de la nouvelle source S
0
est; DS
0 = 300 103 × 0,3726 = 111,78 kbi t s/sec.
Puisque DS
0
DS
= 0,3726, le taux de réduction du débit binaire est de l’ordre de 63%.
4) Le débit d’utilisation du canal étant de 280 kbi t s/sec. Puisque DC
DS
0
=
280
111,78 ' 2,5. Cela veut
dire qu’il faut ajouter 1,5 élément binaire de contrôle à 1 élément binaire d’information, soit 3
éléments binaires de contrôle à 2 éléments binaires d’information.
Pour bien comprendre le raisonnement précédent, il faut imaginer que le canal possède
des tiroirs. Chaque tiroir possède une capacité de 2.5 places. Ainsi, 1 bit source (information) peut occuper une place et les 1,5 places restantes peuvent être utilisés par le codeur
de canal pour les bits de contrôle.
Remarque 7 :
Partie B: Codage du canal
5) (a) Voir tableau.
Mots information Mots code
00 00000
10 01011
10 10101
11 11110
(b) dm = 3, voir mots code.
(c) détection :1 et 2; Correction : 1
(d)
G =
Ã
1 0 1 0 1
| {z }
I2
0 1
| {z }
N
0 1 1
!
(e)
H =




1 0 1 0 0
0 1 0 1 0
| {z }
N
t
1 1
| {z }
I3
0 0 1




37
UV IT4B Théorie de la communication
On remarque que colonne1 +colonne3 = colonne 5, et donc dmin = 3.
(f ) Le syndrome S(y) = H yt = 100, vecteur de correction 00100. Ainsi, le mot reçu y = 11010
est corrigé par : 11010+00100 = 11110.
38